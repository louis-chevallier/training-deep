{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## good old multi-layer perceptron (MLP) from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiny toy problem, no test data, one hidden layer, square loss (solution of \"mlp0\")\n",
    "- play with parameters\n",
    "- monitor training error\n",
    "- replace first sigmoid by ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# main parameters\n",
    "iterNum = 300\n",
    "stepSizes = [0.01,0.1,1,10,100]\n",
    "hiddenSize = 4\n",
    "\n",
    "# Display plots inline \n",
    "%matplotlib inline\n",
    "# Define plot's default figure size\n",
    "matplotlib.rcParams['figure.figsize'] = (2.0, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bounded smooth non-linearity: \n",
    "$\\sigma(x) = \\frac{1}{1+\\exp(-x)},\\quad\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute sigmoid\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input dataset\n",
    "X = np.array([  [0,0,1,1],\n",
    "                [0,1,0,1],\n",
    "                [1,1,1,1] ])\n",
    "trainNum = X.shape[1]\n",
    "\n",
    "# output dataset            \n",
    "Y = np.array([[0,1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fbfc1e25f50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOEAAACaCAYAAABfVi94AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlIr9FycDUM0rhOWwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc147a1a950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize weights randomly with mean 0\n",
    "np.random.seed(1)\n",
    "W1 = 2 * np.random.random((hiddenSize,3)) - 1\n",
    "W2 = 2 * np.random.random((1,hiddenSize)) - 1\n",
    "\n",
    "# visualize weights\n",
    "plt.subplot(121)\n",
    "plt.imshow(W1,interpolation = 'none')\n",
    "plt.axis('off')\n",
    "plt.title('W1')\n",
    "plt.subplot(122)\n",
    "plt.imshow(W2,interpolation = 'none')\n",
    "plt.axis('off')\n",
    "plt.title('W2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "activation (no bias): $a_{\\ell} = Wx_{\\ell-1}$, output: $x_{\\ell} = \\sigma(a_{\\ell})$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "square loss: $\\frac{1}{2N} \\sum_{n=1}^{N} \\|x_2^{(n)}-y^{(n)}\\|^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for step size :0.01 : \n",
      "[[ 0.46693509  0.44372128  0.49290942  0.47199768]]\n",
      "Prediction for step size :0.1 : \n",
      "[[ 0.48749753  0.46884626  0.51438172  0.4961609 ]]\n",
      "Prediction for step size :1 : \n",
      "[[ 0.45307008  0.49488068  0.524805    0.52743277]]\n",
      "Prediction for step size :10 : \n",
      "[[ 0.04999666  0.94300372  0.95292847  0.05559892]]\n",
      "Prediction for step size :100 : \n",
      "[[ 0.01127926  0.98756488  0.99067026  0.01075848]]\n"
     ]
    }
   ],
   "source": [
    "# training with full-batch fixed step-size gradient descent\n",
    "for stepSize in stepSizes:\n",
    "    for iter in range(iterNum):\n",
    "    \n",
    "        # forward propagation on all training data\n",
    "        X0 = X\n",
    "        A1 = np.dot(W1,X0)  \n",
    "        X1 = sigmoid(A1)\n",
    "        A2 = np.dot(W2,X1)\n",
    "        X2 = sigmoid(A2)   \n",
    "        \n",
    "        # back propagation of errors\n",
    "        G2 = X2 - Y\n",
    "        H2  = G2 * sigmoid_output_to_derivative(X2)\n",
    "        G1 = np.dot(W2.T,H2)\n",
    "        H1 = G1 * sigmoid_output_to_derivative(X1)\n",
    "    \n",
    "        W2 -= stepSize * np.dot(H2,X1.T) / trainNum\n",
    "        W1 -= stepSize * np.dot(H1,X0.T) / trainNum\n",
    "    \n",
    "    print(\"Prediction for step size :\" + str(stepSize) + \" : \" )\n",
    "    print(X2)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
